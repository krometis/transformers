{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fbd23e-ca4e-43b3-897d-2ee432a434a9",
   "metadata": {},
   "source": [
    "# Transformer Implementation\n",
    "This notebook contains an example implementation of a transformer model. Some things are omitted, for example:\n",
    "- I did not provide a loss calculation or training loop\n",
    "- I did not include dropout because I was worried the provided code was getting more complicated already\n",
    "- I assume that inputs are already tokenized and embedded\n",
    "\n",
    "For a more complete, but still accessible, implementation, I recommend looking at:\n",
    "- https://github.com/karpathy/nanoGPT (early version)\n",
    "- https://github.com/karpathy/nanochat (newer release)\n",
    "\n",
    "See also the original video here: https://www.youtube.com/watch?v=kCc8FmEb1nY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddbbcf-9d65-475d-886a-7ef0aceb278c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implement Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7b01e-0f0e-4153-8b8f-3cf77c9fdf3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb6bbe-d4fe-493d-b2e4-6d0fde22cb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#As a simple function\n",
    "def attention(q,k,v):\n",
    "    mask    = torch.tril( torch.ones(q.shape[-2],q.shape[-2]) ).bool()\n",
    "    qkT     = torch.matmul( q, k.transpose(-2,-1) ) / (q.shape[-1]**0.5)\n",
    "    qkT_msk = qkT.masked_fill_(~mask, value=float(\"-inf\"))\n",
    "    weights = F.softmax(qkT_msk,-1)\n",
    "    att     = torch.matmul(weights,v)\n",
    "    return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75008e-eda6-4b11-85f1-0ea7e2aead6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#As a PyTorch module\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    '''\n",
    "    Defines a single attention head\n",
    "    '''\n",
    "    def __init__(self, d_embed, d_cntxt, d_k, d_v):\n",
    "        super().__init__()\n",
    "        \n",
    "        # self.d_embed = d_embed\n",
    "        # self.d_cntxt = d_cntxt\n",
    "        self.d_k     = d_k\n",
    "        # self.d_v     = d_v\n",
    "        #trainable maps\n",
    "        self.query = nn.Linear(d_embed,d_k,bias=False)\n",
    "        self.key   = nn.Linear(d_embed,d_k,bias=False)\n",
    "        self.value = nn.Linear(d_embed,d_v,bias=False)\n",
    "        #not trainable mask\n",
    "        self.register_buffer('mask', torch.tril( torch.ones(d_cntxt,d_cntxt) ).bool())\n",
    "        \n",
    "    def attention(self,q,k,v,inspect=False):\n",
    "        qkT     = torch.matmul( q, k.transpose(-2,-1) )\n",
    "        qkT_scl = qkT / torch.sqrt(torch.tensor(self.d_k))\n",
    "        qkT_msk = qkT_scl.clone()\n",
    "        qkT_msk = qkT_msk.masked_fill_(~self.mask, value=float(\"-inf\"))\n",
    "        weights = F.softmax(qkT_msk,-1)\n",
    "        att = torch.matmul(weights,v)\n",
    "        if not inspect:\n",
    "            return att\n",
    "        else:\n",
    "            return { 'qkT':qkT, 'qkT_scl':qkT_scl, 'qkT_msk':qkT_msk, 'weights':weights, 'attention':att }\n",
    "\n",
    "    def forward(self,x):\n",
    "        q = query(x)\n",
    "        k = key(x)\n",
    "        v = value(x)\n",
    "        \n",
    "        return self.attention(q,k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0974ac-68cf-4733-996c-86e5b7e93679",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b631f1-04bf-4e55-b65b-4baeca637d7b",
   "metadata": {},
   "source": [
    "### Explore inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b759470-a655-4a8a-90cd-29729712c6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define various dimensions\n",
    "d_batch = 2   #batch dimension\n",
    "d_embed = 300 #length of embeddings\n",
    "d_cntxt = 8   #context size\n",
    "d_k     = 4   #length of queries and keys\n",
    "d_v     = 4   #length of values\n",
    "\n",
    "n_head  = 4   #number of attention heads to run in parallel\n",
    "n_block = 6   #number of blocks in the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4ccb3-5c2c-4491-86ce-32dfe60023ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define maps between embeddings and queries/keys/values\n",
    "query = nn.Linear(d_embed,d_k,bias=False)\n",
    "key   = nn.Linear(d_embed,d_k,bias=False)\n",
    "value = nn.Linear(d_embed,d_v,bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5718792-3f90-4056-b508-f6f4dae43799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate a random input\n",
    "x = torch.rand(d_batch,d_cntxt,d_embed)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a33cf-7195-4b4b-85d3-7795602ed66d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run query/key/value and look at sizes\n",
    "q, k, v = [ fn(x) for fn in [ query, key, value ] ]\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871aa747-899d-4160-832c-b2dbd3462fb8",
   "metadata": {},
   "source": [
    "### Now run attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f77a37-ab91-41ad-b7f4-31a424a9f1e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate an instance of the above class\n",
    "attnHead = SelfAttentionHead(d_embed,d_cntxt,d_k,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61685a-e111-4838-9274-6261e6655d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#inspect the mask\n",
    "#- values marked with True will be retained\n",
    "#- values marked with False will be zeroed out\n",
    "attnHead.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830b3c7-5534-4337-87d3-abfb450d21e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(attnHead.mask)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/mask.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a480db4-dee7-40b3-bfa2-aed809df6649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run the module\n",
    "out = attnHead(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62ad7f-c0fb-4606-8b83-e7aeda555572",
   "metadata": {},
   "source": [
    "### Inspect steps\n",
    "The `attention` function in the PyTorch module includes an `inspect` option that returns a dictionary of its steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547a4ee-7620-4188-9d0d-997716866ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate i.i.d. inputs from the unit normal distribution\n",
    "q = torch.randn(d_batch,d_cntxt,d_k)\n",
    "k = torch.randn(d_batch,d_cntxt,d_k)\n",
    "v = torch.randn(d_batch,d_cntxt,d_v)\n",
    "\n",
    "#run attention and print keys\n",
    "d = attnHead.attention(q,k,v,inspect=True)\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa6868-1a6f-473d-8755-df435577243a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#QK^T has entries with standard deviation ~d_k\n",
    "d['qkT'].mean(), d['qkT'].var(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904579f3-6f77-47e7-ab08-c1a664a7ecf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#once scaled, standard deviations are closer to 1\n",
    "d['qkT_scl'].mean(-1), d['qkT_scl'].var(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a39e65-5b40-4c5a-943e-61c2b8c5afb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print the weights used to compute the outputs\n",
    "d['weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b660c-144a-4052-a81f-82cb825fd496",
   "metadata": {},
   "source": [
    "### Compare implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b62839-c871-4e2d-8339-d54793707788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#module and function implementations produce the same output\n",
    "at1 = attnHead.attention(q,k,v)\n",
    "at2 = attention(q,k,v)\n",
    "torch.allclose(at1,at2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e7352-9ea0-464f-9102-5041c10bdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#module and PyTorch implementations produce the same output\n",
    "# note: scaled_dot_product_attention() available in PyTorch > 2.0\n",
    "at3 = F.scaled_dot_product_attention(q,k,v,attn_mask=attnHead.mask)\n",
    "torch.allclose(at1,at3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f679ba-8d3d-42f1-9ccd-d10dc715b691",
   "metadata": {},
   "source": [
    "## Finishing the Transformer\n",
    "With attention in place, we now build out the rest of the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77712cd1-ecfe-47e2-bb31-d2d22b898773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    '''\n",
    "    Defines a layer of multihead attention\n",
    "    '''\n",
    "    def __init__(self, n_head, d_embed, d_cntxt, d_k, d_v):\n",
    "        super().__init__()\n",
    "        \n",
    "        #list of attention heads to be run side by side\n",
    "        self.attnHeads = [ SelfAttentionHead(d_embed,d_cntxt,d_k,d_v) for _ in range(n_head) ]\n",
    "        #projection back to the embedding dimension\n",
    "        self.toEmbed   = nn.Linear(d_v*n_head,d_embed)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #run all heads and concatenate\n",
    "        out = torch.cat([ head(x) for head in self.attnHeads],-1)\n",
    "        #run the linear layer to project back to the embedding dimension\n",
    "        out = self.toEmbed(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    '''\n",
    "    Defines a block of a transformer model\n",
    "    '''\n",
    "    def __init__(self, n_head, d_embed, d_cntxt, d_k, d_v, ffw_multiplier=4, ffw_activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        #multi-head attention layer\n",
    "        self.multiHead = MultiHeadAttentionLayer(n_head,d_embed,d_cntxt,d_k,d_v)\n",
    "        \n",
    "        #feedforward layer\n",
    "        self.ffw   = nn.Sequential(\n",
    "            nn.Linear(d_embed,d_embed*ffw_multiplier),\n",
    "            ffw_activation,\n",
    "            nn.Linear(d_embed*ffw_multiplier,d_embed),\n",
    "        )\n",
    "        \n",
    "        #layer normalizations\n",
    "        self.norm1 = nn.LayerNorm(d_embed)\n",
    "        self.norm2 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #run multi-head attention layer\n",
    "        #include residual connection and layer norm\n",
    "        x = self.norm1( x + self.multiHead(x) )\n",
    "        #run feedforward layer\n",
    "        x = self.norm2( x + self.ffw(x) )\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    '''\n",
    "    Defines a transformer model\n",
    "    '''\n",
    "    def __init__(self, n_layer, n_head, d_vocab, d_embed, d_cntxt, d_k, d_v, ffw_multiplier=4, ffw_activation=nn.ReLU(), pe_orig=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        #blocks (each multi-head attention + feedforward)\n",
    "        self.blocks  = nn.Sequential(*[ \n",
    "            TransformerBlock(n_head,d_embed,d_cntxt,d_k,d_v,ffw_multiplier=ffw_multiplier,ffw_activation=ffw_activation) \n",
    "            for _ in range(n_layer) \n",
    "        ])\n",
    "        \n",
    "        #linear layer to return to vocab size\n",
    "        self.toVocab = nn.Linear(d_embed,d_vocab)\n",
    "        \n",
    "        #not trainable mask\n",
    "        self.register_buffer('p_embed', self.pos_embed(d_cntxt,d_embed,original=pe_orig) )\n",
    "\n",
    "    def pos_embed(self,d_cntxt,d_embed,original=True):\n",
    "        p_embed = torch.zeros(d_cntxt,d_embed)\n",
    "        if original:\n",
    "            #PE(pos,2i  ) = sin((pos/10000)^(2i/d_embed))\n",
    "            #PE(pos,2i+1) = cos((pos/10000)^(2i/d_embed))\n",
    "            den = torch.pow(10000,-torch.arange(0,d_embed,2)/d_embed)\n",
    "        else:\n",
    "            #regular fourier basis seems more sane?\n",
    "            den = 2*torch.pi*torch.arange(0,d_embed,2)/d_embed\n",
    "        pos = torch.arange(d_cntxt)\n",
    "        ang = torch.outer(pos,den)\n",
    "        p_embed[:, ::2] = torch.sin(ang)\n",
    "        p_embed[:,1::2] = torch.cos(ang)\n",
    "        return p_embed\n",
    "\n",
    "    def forward(self,x):\n",
    "        #add positional embedding\n",
    "        x = x + self.p_embed\n",
    "        #run blocks\n",
    "        attn_out = self.blocks(x)\n",
    "        #run linear layer\n",
    "        logits = self.toVocab(attn_out)\n",
    "        #convert to probabilities\n",
    "        probs  = F.softmax(logits,-1)\n",
    "        return probs #for training may want to add loss or return logits here\n",
    "    \n",
    "    def predict(self,x):\n",
    "        #get probabilities associated with last entry in context\n",
    "        probs = self(x)[:,-1,:]\n",
    "        #draw to get a prediction (probabilistic!)\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e50a60-d83e-444a-b2ec-53ba449e6f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#multi-head attention\n",
    "multiHead = MultiHeadAttentionLayer(n_head,d_embed,d_cntxt,d_k,d_v)\n",
    "multiHead(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d76b0-7781-4bd4-8890-e2dfa24fd08f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#transformer block\n",
    "trBlock = TransformerBlock(n_head,d_embed,d_cntxt,d_k,d_v,ffw_activation=nn.GELU())\n",
    "trBlock(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c6884-65ed-465b-9c0c-5434c699994c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#transformer block\n",
    "n_layer = 6\n",
    "d_vocab = 10000\n",
    "transformer = TransformerModel(n_layer,n_head,d_vocab,d_embed,d_cntxt,d_k,d_v,ffw_activation=nn.GELU())\n",
    "out = transformer(x)\n",
    "out.shape, out.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56d51a-5070-4405-8f22-f3f40702d669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make a prediction (one for each batch)\n",
    "transformer.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ff821-9aaa-42d0-904d-9404453b1da2",
   "metadata": {},
   "source": [
    "## View Position Embeddings\n",
    "This is a bit of an aside, but since we implemented the position embeddings above, let's visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ae1d8-80e5-48ac-b3f7-125ab44d12e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pe(p_embed,separate=True):\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,4))\n",
    "    for i in range(p_embed.shape[0]):\n",
    "        if separate:\n",
    "            ax.plot(p_embed[i, ::2],label=f'Position {i} (sin)')\n",
    "            ax.plot(p_embed[i,1::2],label=f'Position {i} (cos)')\n",
    "        else:\n",
    "            ax.plot(p_embed[i,   :],label=f'Position {i}')\n",
    "    ax.set_xlabel(\"Embedding Dimension\")\n",
    "    ax.set_ylabel(\"Positional Embedding Value\")\n",
    "    # ax.legend()\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2102e5-6582-4da9-a9dc-b54e7b14f22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#original version - seems like a lot of duplicate information?\n",
    "plot_pe( transformer.pos_embed(d_cntxt,d_embed,original=True) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0de12-f794-42c9-9a5d-ea338f3830cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fourier version\n",
    "plot_pe( transformer.pos_embed(d_cntxt,d_embed,original=False) );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
